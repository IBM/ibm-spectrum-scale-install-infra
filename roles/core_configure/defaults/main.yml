---
# Default variables for the IBM Spectrum Scale (GPFS) cluster role -
# either edit this file or define your own variables to override the defaults


## Specify the Spectrum Scale version that you want to install on your nodes
#scale_version: 4.1.1.0


## Specify the URL of the (existing) Spectrum Scale YUM repository
## (copy the contents of /usr/lpp/mmfs/.../gpfs_rpms to build your repository)
#scale_install_repository_url: http://infraserv/
## Note that if this is a URL then a new repository definition will be created.
## If this variable is set to 'existing' then it is assumed that a repository
## definition already exists and thus will *not* be created.

## Specify the path to Spectrum Scale installation package on the remote system
## (accessible on Ansible managed node)
#scale_install_remotepkg_path: /path/to/Spectrum_Scale_Standard-{{ scale_version }}-{{ ansible_architecture }}-Linux-install

## Specify the path to Spectrum Scale installation package on the local system
## (accessible on Ansible control machine) - it will be copied to your nodes
#scale_install_localpkg_path: /path/to/Spectrum_Scale_Standard-{{ scale_version }}-{{ ansible_architecture }}-Linux-install

## Spectrum Scale daemon nodename (defaults to node's hostname)
scale_daemon_nodename: "{{ ansible_hostname }}"

## Spectrum Scale admin nodename (defaults to node's hostname)
scale_admin_nodename: "{{ scale_daemon_nodename }}"

## Desired state of the Spectrum Scale daemon
scale_state: present

## Default cluster name
scale_cluster_clustername: gpfs1.local

## Node's default quorum role -
## you'll likely want to define per-node roles in your inventory
##
## If you don't specify any quorum nodes then the first seven hosts in your
## inventory will automatically be assigned the quorum role, even if this
## variable is 'false'
scale_cluster_quorum: false

## Node's default manager role -
## you'll likely want to define per-node roles in your inventory
scale_cluster_manager: false


## Default filesystem parameters -
## can be overridden for each filesystem individually
scale_storage_filesystem_defaults:
  blockSize: 1M
  maxMetadataReplicas: 2
  defaultMetadataReplicas: 1
  maxDataReplicas: 2
  defaultDataReplicas: 1
  numNodes: 32
  automaticMountOption: true

  ## defaultMountPoint will be this prefix, followed by the filesystem name
  defaultMountPoint_prefix: /mnt/
  ## Overwrite existing NSDs - if set to 'true' then disks will *not* be checked
  ## for an existing NSD header (dangerous!)
  overwriteNSDs: false
# defaults file for node

# Specifies a predefined profile of attributes to be applied.
# System-defined profiles are located in /usr/lpp/mmfs/profiles/.
# The following system-defined profile names are accepted:
# gpfsprotocoldefaults and gpfsprotocolrandomio
# eg: If you want to apply gpfsprotocoldefaults then specify scale_cluster_profile_name: gpfsprotocoldefaults
scale_cluster_profile_name: None

# Fixed variable related to mmcrcluster profile
# System-defined profiles are located in /usr/lpp/mmfs/profiles/
scale_cluster_profile_dir_path: /usr/lpp/mmfs/profiles/

## Default node role change variable flag
scale_node_role_change: true

## Upgrade check
scale_node_update_check: true

## admin node flag
scale_admin_node: false

## Example storage_cluster_config.yaml file for tunables based on Node Group
# scale_cluster_config:
#   ephemeral_port_range: 60000-61000
# scale_config:
# - nodeclass: storagedescnodegrp
#   params:
#   - maxFilesToCache: 128K
#     maxMBpS: 4000
#     maxReceiverThreads: 8
#     maxStatCache: 128K
#     pagepool: 8G
# - nodeclass: managementnodegrp
#   params:
#   - maxFilesToCache: 128K
#     maxMBpS: 4000
#     maxReceiverThreads: 8
#     maxStatCache: 128K
#     pagepool: 8G
# - nodeclass: storagenodegrp
#   params:
#   - maxFilesToCache: 128K
#     maxMBpS: 4000
#     maxReceiverThreads: 8
#     maxStatCache: 128K
#     pagepool: 8G
# - nodeclass: protocolnodegrp
#   params:
#   - maxFilesToCache: 3M
#     maxMBpS: 4000
#     maxReceiverThreads: 32
#     maxStatCache: 512K
#     pagepool: 16G
# - nodeclass: afmgatewaygrp
#   params:
#   - afmHardMemThreshold: 40G
#     maxFilesToCache: 3M
#     maxMBpS: 16000
#     maxReceiverThreads: 32
#     maxStatCache: 512K
#     pagepool: 32G
